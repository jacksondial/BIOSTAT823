---
title: "bs823HW4"
author: "Jackson Dial"
date: '2022-10-27'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem 1

## A

$$
Z = \sqrt{n}(\overline{X} - \mu)/\sigma \sim N(0,1)
$$

$$
\text{since the distribution is symmetric: } z_{\alpha/2} = -z_{1-\alpha/2}
$$
$$
1- \alpha = P[-z_{1-\alpha/2}< \sqrt{n}(\overline{X} - \mu)/\sigma<z_{1-\alpha/2}]
$$

$$
= P[\overline{X} - z_{1-\alpha /2} \space \sigma/\sqrt{n} < \mu<\overline{X} + z_{1-\alpha/2} \space \sigma/\sqrt{n}]
$$

$$
\text{It follows that a 100(1-} \alpha) \% \text{ CI for } \mu \text{ is given by: }
$$

$$
(\bar{x} - z_{1-\alpha / 2} \space s_n/\sqrt{n}, \bar{x} + z_{1-\alpha/2} \space s_n \sqrt{n})
$$

## B

```{r}
two.sided.ci <- function(xbar, sigma, alpha, n){
  zscore <- qt(1-alpha/2, df = n-1)
  margin_of_error <- zscore * sigma / sqrt(n)
  ub <- xbar + margin_of_error
  lb <- xbar - margin_of_error
  ci <- c(lb, ub)
  return (ci)
}
```

```{r}
ci <- two.sided.ci(xbar = 0, sigma = 1, alpha = .05, n = 100)
ci
```


## C

To create a simulation study for empirical assessment of coverage probability using $N$ simulation replicates for the confidence interval estimator, I would sample many times from a normal distribution for the $\bar{x}$ value. I would calculate a CI for each of the sample means, and then check to see if the true value of the mean of my distribution was contained in the interval. I would calculate the percentage of confidence intervals that contain the true mean, and this would be my coverage probability.

## D

```{r}
set.seed(19)
cov_prob_vec <- vector()
for (i in 1:10000){
  samps <- rnorm(25, 1, .25)
 ci <- two.sided.ci(xbar = mean(samps), sigma = sd(samps), alpha = .05, n = 25)

  if (1 < ci[1] | 1 > ci[2]){
    cov_prob_vec[i] = 0
  }
  else{
    cov_prob_vec[i] = 1
  }
}
 
mean(cov_prob_vec)

```

```{r}
cov_prob_vec2 <- vector()
for (i in 1:10000){
  samps <- rnorm(50, 1, .25)
 ci <-  two.sided.ci(xbar = mean(samps), sigma = sd(samps), alpha = .05, n = 50)
  if (1 < ci[1] | 1 > ci[2]){
    cov_prob_vec2[i] = 0
  }
  else{
    cov_prob_vec2[i] = 1
  }
}

mean(cov_prob_vec2)

```


## E

```{r}
two.sided.bootstrap <- function(xbar, pop_min, alpha, n, pop_max, sigma){
  quant <- qunif(1-alpha/2, min = pop_min, max = pop_max )
  margin_of_error <- quant * sigma / sqrt(n)
  ub <- xbar + margin_of_error
  lb <- xbar - margin_of_error
  ci <- c(lb, ub)
  return (ci)
}
```

```{r}
two.sided.bootstrap(xbar = 3, pop_min = 1.2, alpha = .05, n = 100, pop_max = 5, sigma = 1.3)

```



## F

To run a simulation study using bootstrap methods, I draw a sample N times where that sample consists of individual samples pulled from a population WITH REPLACEMENT. 

## G

```{r}
boot_cov_prob <- vector()
pop <- runif(25, 0, 1)
for (i in 1:10000){
  samps = sample(pop, 1000, replace = TRUE)
 ci <-  two.sided.bootstrap(xbar = mean(samps), pop_min = min(samps), alpha = .05, n = 25, pop_max = max(samps), sigma = sd(samps))
  if (1 < ci[1] | 1 > ci[2]){
    boot_cov_prob[i] = 0
  }
  else{
    boot_cov_prob[i] = 1
  }
}

mean(boot_cov_prob)
```


```{r}
boot_cov_prob2 <- vector()
pop <- runif(50, 0, 1)
for (i in 1:10000){
  samps = sample(pop, 1000, replace = TRUE)
 ci <-  two.sided.bootstrap(xbar = mean(samps), pop_min = min(samps), alpha = .05, n = 50, pop_max = max(samps), sigma = sd(samps))
  if (1 < ci[1] | 1 > ci[2]){
    boot_cov_prob2[i] = 0
  }
  else{
    boot_cov_prob2[i] = 1
  }
}

mean(boot_cov_prob)
```
# Question 2

## A

Likelihood function

$$
L[{\alpha,\beta, \sigma^2}] = -\frac{n}{2}log(2\pi\sigma^2)-\frac{1}{2\sigma^2}\sum_{i=1} ^n (y_i -\alpha+X\beta)^2
$$

## B

Gradient Vector

$$
D_b \space l(b) = 

\begin{bmatrix} \sum_{i=1}^n X_{1i}(y_i-b_1x_{i1} -b_2 x_{2i})\\ \sum_{i=1}^n X_{2i}(y_i-b_1x_{i1} -b_2 x_{2i}) \end{bmatrix}
$$


## C

## D

```{r}
set.seed(1281)
n <- 100
x <- runif(n, 0, 1)

y <- 1 + 0.5 *x + rnorm(n, 0, 0.1)

#plot(x, y, pch = 19, main = "Toy Data Set")

mu = mean(y)
sigma = sd(y)
```

```{python}
import tensorflow as tf
import tensorflow_probability as tfp

model = tfp.distributions.Normal(loc=mu, scale=sigma)
```


```{python}
import tensorflow as tf
dtype = tf.float32
X = tf.constant(r.x, dtype=dtype)
Y = tf.constant(r.y, dtype=dtype)
print(type(X))
print(X.dtype)
print(X.shape)
# extract the first 5 tensors into numpy array
print(X.numpy()[:5])
```


```{python}
import numpy as np
pi = tf.constant(np.pi, dtype=dtype)

@tf.function
def ols_loglike(beta, sigma):
    # xb (mu_i for each observation)
    mu = tf.linalg.matvec(X, beta)
    # this is normal pdf logged and summed over all observations
    ll = - (X.shape[0]/2.)*tf.math.log(2.*pi*sigma**2) -\
	    (1./(2.*sigma**2.))*tf.math.reduce_sum((Y-mu)**2., axis=-1)
    return ll
```


```{python}
ols_loglike(beta = [1., 1.], sigma=1.)
```

```{python}
[funval, grads] = tfp.math.value_and_gradient(ols_loglike, [[1., 1.], 1.])
print("Function Value: ", funval)
print('Gradients on beta: \n', grads[0])
print('Gradient on sigma: \n', grads[1])
```
```{python}
import numpy as np
from scipy.integrate import quad
from scipy.optimize import minimize_scalar
import torch
from matplotlib import pyplot as plt
import pandas as pd
import math 

N = 100
##### SAMPLE
np.random.seed(3)
sample = np.random.normal(loc=5, scale=2, size=(1000, 1))

##### TENSORS
# X = torch.tensor(sample, dtype=torch.float64, requires_grad=False) #sample tensor
X = torch.tensor(np.array([r.y]), dtype = torch.float64, requires_grad = True)
alpha_ = torch.tensor(np.array([0.5]) * N, dtype=torch.float64, requires_grad=True)
beta_ = torch.tensor(np.array([0.5]) * N, dtype=torch.float64, requires_grad=True)
sigma_ = torch.tensor(np.array([5]) * N, dtype=torch.float64, requires_grad=True) 

##### OPTMIZATION METHOD: SGD
learning_rate = 0.0002
OPT_OBJ = torch.optim.SGD([alpha_, beta_, sigma_], lr = learning_rate)

##### OPTIMAZTION METHOD
for t in range(2000):
    # NLL = X.size()[0]*s_.log()+((((X-mu_)/s_ ).pow(2))/2).sum() #negative likelihood UPDATE
    NLL = X.size()[0]*(-N/2) * math.log(2*math.pi * sigma_^2) - (1/s*sigma_^2) * sum((r.y - alpha_ + X*beta_)^2)
    OPT_OBJ.zero_grad()
    NLL.backward()

    if t % 100 == 0:
        print("Log_Likehood: {}; Estimate mu: {}; Estimate sigma: {}".format(NLL.data.numpy(), mu_.data.numpy(), s_.data.numpy()))

    OPT_OBJ.step()

print("True value of mu and sigma: {} e {}".format(5, 2))
```


$$
L[{\alpha,\beta, \sigma^2}] = -\frac{n}{2}log(2\pi\sigma^2)-\frac{1}{2\sigma^2}\sum_{i=1} ^n (y_i -\alpha+X\beta)^2
$$

## E

## F


# Question 3

# Question 4
