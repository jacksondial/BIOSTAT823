---
title: "bs823HW4"
author: "Jackson Dial"
date: '2022-10-27'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem 1

## A

$$
Z = \sqrt{n}(\overline{X} - \mu)/\sigma \sim N(0,1)
$$

$$
\text{since the distribution is symmetric: } z_{\alpha/2} = -z_{1-\alpha/2}
$$
$$
1- \alpha = P[-z_{1-\alpha/2}< \sqrt{n}(\overline{X} - \mu)/\sigma<z_{1-\alpha/2}]
$$

$$
= P[\overline{X} - z_{1-\alpha /2} \space \sigma/\sqrt{n} < \mu<\overline{X} + z_{1-\alpha/2} \space \sigma/\sqrt{n}]
$$

$$
\text{It follows that a 100(1-} \alpha) \% \text{ CI for } \mu \text{ is given by: }
$$

$$
(\bar{x} - z_{1-\alpha / 2} \space s_n/\sqrt{n}, \bar{x} + z_{1-\alpha/2} \space s_n \sqrt{n})
$$

## B

```{r}
two.sided.ci <- function(xbar, sigma, alpha, n){
  zscore <- qt(1-alpha/2, df = n-1)
  margin_of_error <- zscore * sigma / sqrt(n)
  ub <- xbar + margin_of_error
  lb <- xbar - margin_of_error
  ci <- c(lb, ub)
  return (ci)
}
```

```{r}
ci <- two.sided.ci(xbar = 0, sigma = 1, alpha = .05, n = 100)
ci
```


## C

To create a simulation study for empirical assessment of coverage probability using $N$ simulation replicates for the confidence interval estimator, I would sample many times from a normal distribution for the $\bar{x}$ value. I would calculate a CI for each of the sample means, and then check to see if the true value of the mean of my distribution was contained in the interval. I would calculate the percentage of confidence intervals that contain the true mean, and this would be my coverage probability.

## D

```{r}
set.seed(19)
cov_prob_vec <- vector()
for (i in 1:10000){
  samps <- rnorm(25, 1, .25)
 ci <- two.sided.ci(xbar = mean(samps), sigma = sd(samps), alpha = .05, n = 25)

  if (1 < ci[1] | 1 > ci[2]){
    cov_prob_vec[i] = 0
  }
  else{
    cov_prob_vec[i] = 1
  }
}
 
mean(cov_prob_vec)

```

```{r}
cov_prob_vec2 <- vector()
for (i in 1:10000){
  samps <- rnorm(50, 1, .25)
 ci <-  two.sided.ci(xbar = mean(samps), sigma = sd(samps), alpha = .05, n = 50)
  if (1 < ci[1] | 1 > ci[2]){
    cov_prob_vec2[i] = 0
  }
  else{
    cov_prob_vec2[i] = 1
  }
}

mean(cov_prob_vec2)

```


## E

```{r}

two.sided.boot <- function(data, alpha){
  average = mean(data)
  n = length(data)
  ci = quantile(data, probs = c(alpha/2, 1-alpha/2))
  return (ci)
}

test = two.sided.boot(data = c(1,2,4,5,2), alpha = .05)
test
test[1]
```

```{r}
bootcov <- function(data, alpha, B){
  quantile(replicate(B, mean(sample(data,1:length(data), replace = TRUE))), c(alpha/2, 1-alpha/2))
}
pop <- rnorm(25, 1, .25)
bootcov(rnorm(5, 1, .025), .05, 1000L)
```

## F

To run a simulation study using bootstrap methods, I draw a sample N times where that sample consists of individual samples pulled from a population WITH REPLACEMENT. 

## G


```{r}
boot_cov_prob <- vector()

for (i in 1:10000){
  # samps = sample(pop, 1000, replace = TRUE)
  # ci <-  two.sided.boot(data = samps, alpha = .05)
  pop <- rnorm(25, 1, .25)
  ci <- bootcov(data = pop, alpha = .05, B = 1000L)
  if (1 < ci[1] | 1 > ci[2]){
    boot_cov_prob[i] = 0
  }
  else{
    boot_cov_prob[i] = 1
  }
}

mean(boot_cov_prob)
```

```{r}
boot_cov_prob2 <- vector()

for (i in 1:10000){
  # samps = sample(pop, 1000, replace = TRUE)
  # ci <-  two.sided.boot(data = samps, alpha = .05)
  pop <- rnorm(50, 1, .25)
  ci <- bootcov(data = pop, alpha = .05, B = 1000L)
  if (1 < ci[1] | 1 > ci[2]){
    boot_cov_prob2[i] = 0
  }
  else{
    boot_cov_prob2[i] = 1
  }
}

mean(boot_cov_prob2)
```


# Question 2

## A

Likelihood function

$$
L[{\alpha,\beta, \sigma^2}] = -\frac{n}{2}log(2\pi\sigma^2)-\frac{1}{2\sigma^2}\sum_{i=1} ^n (y_i -\alpha+X\beta)^2
$$

## B

Gradient Vector

$$

D_b \space l(b) = 

\begin{bmatrix}
\frac{1}{\sigma^2}[\sum_{i=1}^ny_i  - n\alpha + \beta\sum_{i=1}^nx_i]
\\ 
-\frac{1}{\sigma^2}[\beta\sum_{i=1}^nx_i^2 +\alpha\sum_{i=1}^n-\sum_{i=1}^nx_iy_i]
\\
-\frac{n}{\sigma}+\frac{\sum_{i=1}^n (y_i-\alpha-\beta x_i)^2}{\sigma^3}
\end{bmatrix}

$$


## C

Find MLE of alpha, beta, sigma^2

## D

```{python}
import numpy as np

np.random.seed(34)
n = 100
x = np.random.normal(0,1,size=n)

y = 1 + 0.5 * x + np.random.normal(0,.1,size = n)

# plot(x, y, pch = 19, main = "Toy Data Set")
```


```{python}
import numpy as np
from scipy.integrate import quad
from scipy.optimize import minimize_scalar
import torch
from torch.autograd import Variable
from matplotlib import pyplot as plt
import pandas as pd
import math 

N = 100
##### SAMPLE
np.random.seed(3)
sample = np.random.normal(loc=5, scale=2, size=(1000, 1))

##### TENSORS
# X = torch.tensor(sample, dtype=torch.float64, requires_grad=False) #sample tensor
Y = Variable(torch.from_numpy(y)).type(torch.FloatTensor)
X = Variable(torch.from_numpy(x)).type(torch.FloatTensor)
alpha_ = torch.tensor(np.array([0.5]), dtype=torch.float64, requires_grad=True)
beta_ = torch.tensor(np.array([0.5]), dtype=torch.float64, requires_grad=True)
sigma_ = Variable(torch.rand(1), requires_grad = True)

##### OPTMIZATION METHOD: SGD
learning_rate = 0.00001
OPT_OBJ = torch.optim.SGD([alpha_, beta_, sigma_], lr = learning_rate)

##### OPTIMAZTION METHOD
for t in range(1000):
    NLL = (N/2) * math.log(2*math.pi * sigma_.pow(2)) + (1/2*sigma_.pow(2)) * sum(((Y - alpha_ - X*beta_).pow(2)))
    OPT_OBJ.zero_grad()
    NLL.backward()

    # if t % 100 == 0:
    #     print("Log_Likehood: {}; Estimate mu: {}; Estimate sigma: {}".format(NLL.data.numpy(), mu_.data.numpy(), s_.data.numpy()))

    OPT_OBJ.step()

# print("True value of mu and sigma: {} e {}".format(5, 2))

torch_alpha = alpha_.data.numpy()[0]
torch_beta = beta_.data.numpy()[0]
torch_var = sigma_.data.numpy()[0]**2

print(torch_alpha)
print(torch_beta)
print(torch_var)
```


## E

```{r}
# 
# /**/
# My Rcpp still won't knit so I am commenting it out again :(
# 
# Just kidding, my file won't even Knit at all if I have an Rcpp chunk in it
# 
# // [[Rcpp::depends(RcppEigen)]]
# // [[Rcpp::depends(RcppNumerical)]]
# 
# #include <RcppNumerical.h>
# 
# using namespace Numer;
# 
# // f = 100 * (x2 - x1^2)^2 + (1 - x1)^2
# // True minimum: x1 = x2 = 1
# class MSE: public MFuncGrad
# {
# public:
#     double f_grad(Constvec& x, Refvec grad)
#     {
#         double t1 = x[1] - x[0] * x[0];
#         double t2 = 1 - x[0];
#         double t3 = 
#         grad[0] = -400 * x[0] * t1 - 2 * t2;
#         grad[1] = 200 * t1;
#         return 100 * t1 * t1 + t2 * t2;
#     }
# };
# 
# // [[Rcpp::export]]
# Rcpp::List optim_test()
# {
#     Eigen::VectorXd x(2);
#     x[0] = -1.2;
#     x[1] = 1;
#     double fopt;
#     Rosenbrock f;
#     int res = optim_lbfgs(f, x, fopt);
#     return Rcpp::List::create(
#         Rcpp::Named("xopt") = x,
#         Rcpp::Named("fopt") = fopt,
#         Rcpp::Named("status") = res
#     );
# }
# 

```


## F

Well, the pytorch way seems to be easier given that I actually know python, but other than that they are pretty similar in functionality.

# Question 3

See other document for parts A & B

## C

```{r}
bayeserror <- function(cc, n) {
  x <- runif(n, 0, 4*cc)
  etax <- x/(x+cc)
  Y <- rbinom(n, 1, etax)
  Yhat <- as.integer(etax > 0.5)
  mean(Y!=Yhat)
}

bayeserror(.25, 10000)

```
Yay! We got the same value!!

# Question 4

See other document
